{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654f0d56-433f-4ed1-b37b-d4986c99f78a",
   "metadata": {},
   "source": [
    "# Missing values - Feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d36d54-6f50-42b1-a822-4cb0c1e7c741",
   "metadata": {},
   "source": [
    "### Lifecycle of data cycle \n",
    "\n",
    "1 . Data collection : from companys' side, from 3rd party api, surveys.\n",
    "2 .  EDA\n",
    "3. Feature engineering - 1. HHndling missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f6be9-2d99-42c7-961b-1791689acc26",
   "metadata": {},
   "source": [
    "why are there missing values??\n",
    "\n",
    "1. people would not provide information (hesitate to)\n",
    "2. quickly fill the forms, so error data gets recorded.. etc..\n",
    "\n",
    "We cannot depend on this data completely as they are not valid all the time.\n",
    "Hence, data should be collected from multiple sources and combine them. It very important.\n",
    "\n",
    "Data that will be missing:\n",
    "\n",
    "1 .  Numerical data - Discrete or continuous \n",
    "\n",
    "2 . Categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad305cf1-ec35-42f1-8a4f-3b4c672987e8",
   "metadata": {},
   "source": [
    "# What we should always be checking when doing imputation in null values :\n",
    "\n",
    "1. Check the dustribution of data before and after filling the null values.\n",
    "2. check the standard deviation of data before and after.\n",
    "3. Correlation with other variables before and after\n",
    "3. check outliers before and after."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1e6c7-5cc5-47fd-aa3f-99f495f753f9",
   "metadata": {},
   "source": [
    "# Ways to handle null values in numerical data :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7ddf2-0d5f-4295-9446-0ada9248c973",
   "metadata": {},
   "source": [
    "what are different types of missing data?\n",
    "\n",
    "Here, to understand below type, sometimes we need to have domain knowledge.\n",
    "\n",
    "1. MCAR (Missing Completely at Random) : \n",
    "\n",
    "If the probability od being missing is same for all the observations. There is no relation betwn data missing and any other values. Those missing values are random subset of data.\n",
    "For example - Check titanic dataset. In that age, cabin , embarked(from which station ppl are picked up or dropped). In this case, age or cabin of people is not available as they are dead and there is no one to provide that info. So its not missing completely at ranndom, there is a reason behind this values being missed. But for embarked, 2 values are missing and there is no idea why this values are missing. So, embarked is example of missing completely at random, but age and cabin are not.  \n",
    "\n",
    "It means the column whose values are missing are not dependent on any of the feature mentioned in data. Example, ppl didnt put their salary because of technical isse. Now technical issue is not the part of data or any features in data, so its MCAR.\n",
    "\n",
    "\n",
    "2. Missing Not At Random (MNAR) : systematic missing values.\n",
    "\n",
    "There is absolute some relationship between missing values and other observed values within the dataset. Now cabin and age is the example of this type, as there is some relation behind their missingness.\n",
    "\n",
    "Here, the column whose values are missing on that column itself (example, ppl with higher or lower salary dont mention salary as its more or less, so not putting the salary depends on the salary itself)\n",
    "\n",
    "   \n",
    "\n",
    "3. MAR (Missing At Random) :\n",
    "\n",
    " Column whose values are missing depends on other columns which are present in data. Example, young ppl not putting their salaries because of their age. Now age also is present in data, and young pppl not putting salary depends on their age(technically missing variable depends on other variable present in data). this is missing at random.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816897d-551d-4346-99da-7506a84e1d3b",
   "metadata": {},
   "source": [
    "# All techniques to handle missing values.\n",
    "\n",
    "1. Mean, median, mode\n",
    "2. Random sample imputation\n",
    "3. Capturing NAN values with a new feature\n",
    "4. End of distribution imputation\n",
    "5. Arbitrary imputation\n",
    "6. Frequent category imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875fe66-0860-459b-99ae-2422cf9ae624",
   "metadata": {},
   "source": [
    "# 1. Mean, Median, Mode imputation\n",
    "\n",
    "When should we apply this??  - It has the assumption that the data are missing completely at random (MCAR)\n",
    "How do we solve this? - we take mean/median/mode of data to fill the missing values. \n",
    "\n",
    "When should we use mean/ median/mode:\n",
    "\n",
    "Mean - If there are no outliers in data, we can use mean of data to impute null values as mean is very sensitive to outliers.\n",
    "\n",
    "Median - If there are outliers present in data, we can use use median as it is not affected by outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29827d1-ecca-4791-8de5-a31373156c8d",
   "metadata": {},
   "source": [
    "# Advantages and disadvantages of mean/median imputation :\n",
    "\n",
    "Advantages :\n",
    "\n",
    "1. Easy to implement\n",
    "\n",
    "2. Faster way to get the entire dataset\n",
    "\n",
    "Disadvantages : \n",
    "\n",
    "1. change in variance of original data.\n",
    "2. Impact on correlation     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9895d5f-94a4-457b-ad5d-4e4d01303287",
   "metadata": {},
   "source": [
    "# 2. Random Sample Imputation :\n",
    "\n",
    "Its similar to mean/median imputation which consists of taking random observation from the dataset itself and use that to replace the null/NAN values.\n",
    "\n",
    "when should we use that?? - It assumes that data is missing completely at random (MCAR)\n",
    "\n",
    "Advantages :\n",
    "\n",
    "1. Easy to implement\n",
    "2. It solves the disrtotion or change in variance problem we face in mean/median imputation.\n",
    "\n",
    "Disadvantages : \n",
    "\n",
    "1. Randomness wont work in every situation. Just try and check it its working properly or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befae7c6-923f-440c-8e30-82869bf80856",
   "metadata": {},
   "source": [
    "# 3. Capturing NAN values with a new feature/variable : \n",
    "\n",
    "When should we use this?? : It works well when data is not missing at random.\n",
    "\n",
    "In this method, we create the new feature of the column which has missing values, and in this new feature, we replace null values with 1 and remaining values with 0\n",
    "\n",
    "we do this, so that model would give more importance to null values. We just create this new feature to add the importance to the missingness.\n",
    "\n",
    "After creating this new column, we should impute null values in original column with some method like mean/median , random sample etc.. When we do thism model try to understand, that ok, whenever there is 1 in this new column, there is this same median value (in case of median/mean imputation) or this some random sample value(in case of random sample imputation)..(we can try other imputation too).\n",
    "Model would just try to understand some relation if we create this new column and  give 1 to null values. Thats why we create this new column.\n",
    "\n",
    "Advantages: \n",
    "1. Easy to implement\n",
    "2. Captures the importance of null values.\n",
    "\n",
    "Disadvantages : \n",
    "\n",
    "1. Its creating addition features. If you have 10 columns which has missing values, we would have to create new 10 columns and hence this may lead to curse of dimensionality. If data has less features with null values, then we can use this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5718845c-bd01-42d4-b82c-3e8c7ef8846f",
   "metadata": {},
   "source": [
    "# What is curse of dimensionality?? -- Just for information purpose. We would see this in machine learning play list.\n",
    "\n",
    "Lets consider that we have 10 models which we are training with different numbers of features. We started with 1 ,2 ,3 features for model 1 ,2 3 respectively.\n",
    "WEe trained model 1 with 1 independent feature then model 2 with 2 independent features, model 3 with three independent features. As we keep on increasing features, the model would give better accuracy. But it would happen till a threshold. It means, till certain number of features model would give better accuracy and after that threshold limit of features , if we add more features, our models accuracy would start decreasing. This is called curse of dimensionality.\n",
    "\n",
    "Why does it happen? As we keep on increasing features till the threshold values, model learns alot from features. Model tries to learn more unique information from the data. But it we start increasing the features exponentially, the model gets confused with a lot of information which is being fed to it. It will not be able to observe all the information present in data, hence the accuracy decreases. Hence, if features keep on increasing explonentially, the models accuracy would start getting decreased.\n",
    "\n",
    "We use many tests to understand what is the threshold and how many features we should be using."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "900293f1-af1c-4b32-a3c6-7a2b495e85a5",
   "metadata": {},
   "source": [
    "# 4. End of distribution imputation.\n",
    "\n",
    "This is also an another way to capture the **importance of missingness.**\n",
    "\n",
    "When do we use this?? - Missing Not at random.\n",
    "\n",
    "How does it work?? - Here, to impute null values, we would use some value which is present at the end/tail of distribution of the feature data. This value we are going to choose is after the 3rd standrad deviation. Check the normal distribution image. In that check 3sigma, we would take the value after that to impute null values here. How do we calculate that value - **df[feature].mean() + 3*df[feature].std()**\n",
    "\n",
    "when we perform this, in some scenarios the outliers would be handled properly. Your new column/ feature you would generate after imputation would not have outliers. It would work in some scenarios and not in some!. If missing value is not at not at random, then it is necessary to capture that information, at that time we use this technique.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "1. Easy to implement\n",
    "2. Captures the importance of missingness is there is one.\n",
    "\n",
    "Disadvantages:\n",
    "1. Distorts the distribution of data.\n",
    "2. It would change the varuance/stf too.\n",
    "3. If missingness is not important, it would mask the predictive power of original variable by distorting the distribution.\n",
    "4. If number of NAs are big, it would mask true outliers in the distribution\n",
    "5. If number of NAs are small, the replaced NA might be considered as an outlier and would be preprocessed in subsequent steps in feature engineering \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246cbb66-1d4c-402d-8f62-4bf755e73886",
   "metadata": {},
   "source": [
    "# Arbitrary value imputation\n",
    "\n",
    "How exactly it works : We just fill null values with some arbitary values to give importance to null values. So in this case, we take extreme outliers to fill this null values. This outlier can be the smallest value in the data or largest value in data or maybe value larger than the max value or value smaller than the min value. Just some extreme value we do take. \n",
    "\n",
    "Arbitrary value choice means personal choice \n",
    "\n",
    "While choosing the Arbitrary value make sure that it is not  more frequently present in dataset (like outliers)- (last outlier or least outlier value)\n",
    "\n",
    "It is used to find the importance of missing values.\n",
    "\n",
    "It works in some scenarios. Just try using this technique and check before and after imputation changes.\n",
    "\n",
    "\n",
    "Advantages:\n",
    "\n",
    "1. Easy to implement\n",
    "2. Captures the importance of missingness is there is one.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1. Distorts the distribution of data.\n",
    "2. If missingness is not important, it would mask the predictive power of original variable by distorting the distribution.\n",
    "3. Hard to decide when to use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbed57-2831-498e-9042-7011b2743ed5",
   "metadata": {},
   "source": [
    "## Remember that in most scenarios we use first 2 techniques in case of numerical values null imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282c8b1-7c69-4db8-9426-6d623d14dddf",
   "metadata": {},
   "source": [
    "# Handle missing values in categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3218e8a-9f51-4d1c-9bb6-cc03ddbaba7b",
   "metadata": {},
   "source": [
    "# 1. Frequent category imputation\n",
    "\n",
    "when columns have very few missing values, at that time, we can use this method to impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5a077-1a16-4885-bd5a-fa815c23c87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee38635-8ff0-4395-bf82-a5259df5509e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97adf6-581d-480c-8874-326973862de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
